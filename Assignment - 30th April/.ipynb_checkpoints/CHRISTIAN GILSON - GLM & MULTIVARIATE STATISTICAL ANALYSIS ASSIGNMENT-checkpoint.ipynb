{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q1)**\n",
    "\n",
    "> Study on infant respiratory disease\n",
    "\n",
    "> Dependent variable: binary category (1/0 flag) for infants on whether or not they developed the disease in the first year of their life (this is the dependent variable)\n",
    "\n",
    "> Explanatory variables: **(1)** genders of each infant (binary category boy/girl); **(2)** how fed (three categories: breast-fed, bottle-fed, supplement)\n",
    "\n",
    "Fitted model in R:\n",
    "\n",
    "```R\n",
    "fit <- glm(disease/(disease + nondisease) ~ gender + food,\n",
    "family = binomial, weights = disease + nondisease, data=babyfood)\n",
    "```\n",
    "\n",
    "### a) State the model that has been fitted.\n",
    "\n",
    "The model fitted is a **linear regression model**. This model has been implemented in R via GLM using the binomial family with no link function specified. As no link function has been explicitly called, R will have used the canonical link function for the binomial distribution, which is the logit link function. The weights, $w_{i}$, are of the linear regression model are total number of infants with and without the disease.\n",
    "\n",
    "$g_{\\text{canonical}}(\\pi_{i}) = g_{\\text{logit}}(\\pi_{i}) = \\ln\\frac{\\pi_{i}}{(1 - \\pi_{i})}$\n",
    "\n",
    "### b) Interpret the significance of the parameters for each explanatory variable.\n",
    "\n",
    "The **girl** parameter for the **gender** explanatory variable is significant at the 5% level, suggesting that gender plays a significant role in whether or not an infant develops respiratory disease in the first year of their lives. The **breast-fed** parameter for the **food** explanatory variable is highly significant, with a $p$-value of $1.22 \\times 10^{-5}$ meaning the parameter is significant at the 0.1% level.\n",
    "\n",
    "The **supplement** parameter has a large standard error with respect to its parameter estimate, and thus has a small $z$-score which is not significant at the 5% or even the 10% level. Therefore the model suggests that taking food suppliments has no significant effect on whether an infant develops respiratory disease in the first year of their life.\n",
    "\n",
    "When dealing with categorical variables in R, one of the classes of the variable is included in the intercept. With us having two categorical variables, *one of the classes from each variable* would have been included in the intercept. For **gender** this would have been the class **boy**, and for **food** this would have been the class **bottle-fed**. This boy / bottle-fed hybrid baseline parameter is highly significant, with a $p$-value below $2 \\times 10^{-16}$, and is therefore clearly significant at the 0.1% significance level. To untangle these parameters from each other to help with our interpretation of the model, we could choose a different class order for each explanatory variable, and thus have different parameters contribute towards the intercept parameter.\n",
    "\n",
    "### c) Does the model provide a good fit?\n",
    "\n",
    "By looking at the residual deviance of the model in R which represents the model deviance, and knowing that the scale parameter for a binomial GLM, $\\phi$, is equal to 1, we know that the residual deviance = deviance, $D$ = **scaled deviance**, $S$, and thus we can use the residual deviance in a goodness-of-fit test:\n",
    "\n",
    "$S = D/ \\phi = D \\sim \\chi^{2}_{n-p}$\n",
    "\n",
    "Plugging in the scaled deviance of $0.72192$ and the degrees of freedom of $6-4=2$, we produce the model $p$-value from the following R code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.697006878184402"
      ],
      "text/latex": [
       "0.697006878184402"
      ],
      "text/markdown": [
       "0.697006878184402"
      ],
      "text/plain": [
       "[1] 0.6970069"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deviance <- 0.72192\n",
    "\n",
    "n <- 6\n",
    "\n",
    "p <- 4\n",
    "\n",
    "df.n.sub.p <- n-p\n",
    "\n",
    "pchisq(deviance, df.n.sub.p, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a $p$-value of $0.70$ we do not have sufficient evidence to reject the null hypothesis comparing the current model against the saturated model with $n-p$ additional parameters with respect to the current model, and therefore we accept the null hypothesis that the $n-p$ additional parameters in the saturated model are equal to zero, and hence that our model does not require additional parameters. Our model is deemed adequete.\n",
    "\n",
    "**We can therefore say that the model provides a good fit**.\n",
    "\n",
    "It's interesting to look at the explanatory variable contributions to the deviance in the ANOVA table, and it's clear that the **food** variable has the largest and most significant impact of the two variables in reducing the residual deviance.\n",
    "\n",
    "As we search for an adequete model that's also parsimonious, we may wish to re-fit the model without **gender** and see how the $p$-value of the model changes.\n",
    "\n",
    "### d) R code to calculate p-value of the model\n",
    "\n",
    "As shown in **c)**, the code to calculate the $p$-value for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.697006878184402"
      ],
      "text/latex": [
       "0.697006878184402"
      ],
      "text/markdown": [
       "0.697006878184402"
      ],
      "text/plain": [
       "[1] 0.6970069"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pchisq(deviance, df.n.sub.p, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Interpret the effect of breast-feeding and derive a 95% confidence interval for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value associated with breast-feeding effect, with an estimated coefficient of $-0.6693$ and a standard error in that estimate of $0.1530$ produces a $z$ statistic of $-4.374$, which is highly significant.\n",
    "\n",
    "One can calculate the $p$-value (given in the R summary) associated with that $z$ statistic is $1.22 \\times 10^{-5}$, and thus the breast feeding effect is significant at the 1% level using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1.21990378334922e-05"
      ],
      "text/latex": [
       "1.21990378334922e-05"
      ],
      "text/markdown": [
       "1.21990378334922e-05"
      ],
      "text/plain": [
       "[1] 1.219904e-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "2*pnorm(-4.374, lower.tail = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of standard errors to produce a 95% confidence interval is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1.95996398454005"
      ],
      "text/latex": [
       "1.95996398454005"
      ],
      "text/markdown": [
       "1.95996398454005"
      ],
      "text/plain": [
       "[1] 1.959964"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using the standard normal distribution\n",
    "num.se.95 <- qnorm(0.025, lower.tail = FALSE)\n",
    "\n",
    "num.se.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a well-known limit: 1.96 standard errors produces a 95% confidence interval for the standard normal distribution.\n",
    "\n",
    "We can now plug in these values to produce the 95% confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>-0.969174489634628</li>\n",
       "\t<li>-0.369425510365372</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item -0.969174489634628\n",
       "\\item -0.369425510365372\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. -0.969174489634628\n",
       "2. -0.369425510365372\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] -0.9691745 -0.3694255"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameter.estimate <- -0.6693\n",
    "parameter.se <- 0.1530\n",
    "\n",
    "parameter.estimate + c(-num.se.95*parameter.se,num.se.95*parameter.se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the 95% confidence interval for the breast feeding coefficient is:\n",
    "\n",
    "> $[-0.969, -0.369]$\n",
    "\n",
    "### f) Estimate an unbiased estimator of the scale parameter, $\\phi$.\n",
    "\n",
    "I.e. what estimator provides: $E[\\hat{\\phi}] = \\phi$?\n",
    "\n",
    "We can use the property that the expectation of a $\\chi^{2}$ random variable is the number of degrees of freedom of that random variable.\n",
    "\n",
    "If \n",
    "\n",
    "> $S = \\frac{D}{\\phi} \\sim \\chi^{2}_{n-p}$,\n",
    "\n",
    "then \n",
    "\n",
    "> $E[\\frac{D}{\\phi}] = n-p$,\n",
    "\n",
    "and therefore\n",
    "\n",
    "> $E[\\frac{D}{n-p}] = \\phi$\n",
    "\n",
    "and thus $\\frac{D}{n-p}$ is an unbiased estimator for $\\phi$:\n",
    "\n",
    "> $\\hat{\\phi} = \\frac{D}{n-p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.36096"
      ],
      "text/latex": [
       "0.36096"
      ],
      "text/markdown": [
       "0.36096"
      ],
      "text/plain": [
       "[1] 0.36096"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale.estimate <- deviance / df.n.sub.p\n",
    "scale.estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\hat{\\phi} = 0.36096$\n",
    "\n",
    "As a sanity check, we'd expect this to be fairly close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Which model would be most reasonable to endorse between:\n",
    "\n",
    "```r\n",
    "fit2 <- glm(disease/(disease + nondisease) ~ gender + food,\n",
    "family = binomial(link=\"identity\"), weights = disease + nondisease)\n",
    "```\n",
    "\n",
    "*and*\n",
    "\n",
    "```r\n",
    "fit3 <- glm(disease/(disease + nondisease) ~ gender + food,\n",
    "family = binomial(link=\"probit\"), weights = disease + nondisease)\n",
    "```\n",
    "\n",
    "The only difference between the two models is the choice of link function. I think the **fit3** model with the **probit** link function would be the superior model Vs the **fit2** model which uses the **identity** link function.\n",
    "\n",
    "Link functions are the mathematical machinery within GLMs to fit non-linear models, mapping the mean value for observation $i$ (i.e. the prediction) to the linear predictor, $\\eta_{i}$, via $g(\\mu_{i}) = \\eta_{i}$. The **identity** link function, as the name suggests, provides no non-linearity in the mapping between $\\mu_{i}$ and $\\eta_{i}$, whereby $\\mu_{i} = \\eta_{i}$, such as is the case in the general linear model.\n",
    "\n",
    "Since we have seen the non-linear **logit** link function perform well in the above goodness-of-fit tests, we expect that a non-linear S-shaped function, like the **probit** link function, will fit the data better than a linear fit via the **identity** link.\n",
    "\n",
    "Secondly (but perhaps most importantly), the **probit** link function has a range between $0$ and $1$ which is essential for fitting a model to a **proportion** with a range between $0$ and $1$, rather than the linear **identity** function with a range from $- \\infty$ to $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h) For the following model, what is the deviance, $D$, on $d$ degrees of freedom? And what is $d$?\n",
    "\n",
    "```r\n",
    "fit4 <- glm(disease/(disease + nondisease) ~ gender*food,\n",
    "family = binomial, weights = disease + nondisease)\n",
    "```\n",
    "\n",
    "Using the ```gender*food``` rather than ```gender+food``` syntax, we're adding the following two interaction terms to the model (in addition to the model already fit with $2$ degrees of freedom):\n",
    "\n",
    "* ```genderGirl:foodBreast```;\n",
    "* ```genderGirl:foodSuppl```.\n",
    "\n",
    "Each of the two interaction terms requires a single degree of freedom.\n",
    "\n",
    "We will therefore have zero degrees of freedom remaining: $d = 0$.\n",
    "\n",
    "This means that $n-p = 0$ and therefore that $p$, the number of parameters in the **fit4** model, is equal to the number of observations, $n$. **fit4** is therefore the saturated model.\n",
    "\n",
    "This will result in us fitting the saturated model, and thus comparing the saturated model (**fit4**) Vs the saturated model in our hypothesis test, and therefore our generalised LRT, $\\Lambda$, will equal 1, and therefore our scaled deviance $S = -2\\ln\\Lambda$ will equal 0, as $\\ln 1 = 0$.\n",
    "\n",
    "Finally, since the scaled deviance, $S = 0$, then the deviance, $D = 0$.\n",
    "\n",
    "* $d = 0$\n",
    "* $D = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Classification rule devivation\n",
    "\n",
    "Starting with some definitions:\n",
    "\n",
    "* $\\pi_{i}$ is the prior probability that an individual selected at random belongs to population $i$.\n",
    "* $C(i|j)$ is the cost of incorrectly allocating an individual to population $i$, when they really belong to population $j$.\n",
    "\n",
    "**Bayes rule**:\n",
    "\n",
    "Allocate population 1 if:\n",
    "\n",
    "> $\\dfrac{f_{1}(\\mathbf{x})}{f_{2}(\\mathbf{x})} \\geq \\dfrac{\\pi_{2} C(1|2)}{\\pi_{1}C(2|1)}$,\n",
    "\n",
    "otherwise allocate population 2.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "In the multivariate normal case (with $p$ variables), where the observation vectors $\\mathbf{X}_{i} \\sim \\text{MVN}_{p}(\\mathbf{\\mu_{i}}, \\Sigma)$, then our two probability density functions for population 1 and 2 are:\n",
    "\n",
    "> $f_{1}(\\mathbf{x}) = \\dfrac{1}{{\\left |2 \\pi \\Sigma \\right |}^{1/2}} \\exp\\left \\{-\\dfrac{1}{2}(\\mathbf{x} - \\mathbf{\\mu_{1}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{1}})   \\right \\}$,\n",
    "\n",
    "and\n",
    "\n",
    "> $f_{2}(\\mathbf{x}) = \\dfrac{1}{{\\left |2 \\pi \\Sigma \\right |}^{1/2}} \\exp\\left \\{-\\dfrac{1}{2}(\\mathbf{x} - \\mathbf{\\mu_{2}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{2}})   \\right \\}$,\n",
    "\n",
    "respectively.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "We can therefore calculate the likelihood ratio as:\n",
    "\n",
    "> $\\dfrac{f_{1}(\\mathbf{x})}{f_{2}(\\mathbf{x})} = \\exp\\left \\{-\\dfrac{1}{2}(\\mathbf{x} - \\mathbf{\\mu_{1}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{1}}) + \\dfrac{1}{2}(\\mathbf{x} - \\mathbf{\\mu_{2}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{2}})  \\right \\}$,\n",
    "\n",
    "where the $\\dfrac{1}{{\\left |2 \\pi \\Sigma \\right |}^{1/2}}$ terms in each pdf cancel out.\n",
    "\n",
    "We can then calculate the log-likelihood by taking natural logs of both sides:\n",
    "\n",
    "> $\\ln \\left ( \\dfrac{f_{1}(\\mathbf{x})}{f_{2}(\\mathbf{x})} \\right ) = -\\dfrac{1}{2}(\\mathbf{x} - \\mathbf{\\mu_{1}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{1}}) + \\dfrac{1}{2}(\\mathbf{x} - \\mathbf{\\mu_{2}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{2}})$\n",
    "\n",
    "and factorising the $\\dfrac{1}{2}$:\n",
    "\n",
    "> $\\ln \\left ( \\dfrac{f_{1}(\\mathbf{x})}{f_{2}(\\mathbf{x})} \\right ) = -\\dfrac{1}{2} \\left ( (\\mathbf{x} - \\mathbf{\\mu_{1}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{1}}) -(\\mathbf{x} - \\mathbf{\\mu_{2}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{2}}) \\right )$.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "We can expand out the $(\\mathbf{x} - \\mathbf{\\mu_{1}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{1}}) -(\\mathbf{x} - \\mathbf{\\mu_{2}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{2}})$ terms:\n",
    "\n",
    "Firstly the $(\\mathbf{x} - \\mathbf{\\mu_{1}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{1}})$ term:\n",
    "\n",
    "> $= (\\mathbf{x}^{T}\\Sigma^{-1} - \\mathbf{\\mu_{1}}^{T}\\Sigma^{-1})(\\mathbf{x} - \\mathbf{\\mu_{1}})$\n",
    "\n",
    "> $= \\mathbf{x}^{T}\\Sigma^{-1}\\mathbf{x} - \\mathbf{x}^{T}\\Sigma^{-1}\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{1}}^{T}\\Sigma^{-1}\\mathbf{x} + \\mathbf{\\mu_{1}}^{T}\\Sigma^{-1}\\mathbf{\\mu_{1}}$\n",
    "\n",
    "Each of these four components is a scalar, and the transpose of a scalar is the scalar itself, i.e. $a^{T} = a$, therefore:\n",
    "\n",
    "> $\\mathbf{\\mu_{1}}^{T}\\Sigma^{-1}\\mathbf{x} = \\left ( \\mathbf{\\mu_{1}}^{T}\\Sigma^{-1}\\mathbf{x} \\right )^{T} = \\mathbf{x}^{T}(\\Sigma^{-1})^{T}\\mathbf{\\mu_{1}} = \\mathbf{x}^{T}(\\Sigma^{T})^{-1}\\mathbf{\\mu_{1}} = \\mathbf{x}^{T}\\Sigma^{-1}\\mathbf{\\mu_{1}}$,\n",
    "\n",
    "since $\\Sigma$ is a symmetric matrix, $\\Sigma = \\Sigma^{T}$.\n",
    "\n",
    "We can therefore write:\n",
    "\n",
    "> $(\\mathbf{x} - \\mathbf{\\mu_{1}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{1}}) = \\mathbf{x}^{T}\\Sigma^{-1}\\mathbf{x} - 2\\mathbf{x}^{T}\\Sigma^{-1}\\mathbf{\\mu_{1}} + \\mathbf{\\mu_{1}}^{T}\\Sigma^{-1}\\mathbf{\\mu_{1}}$\n",
    "\n",
    "and\n",
    "\n",
    "> $(\\mathbf{x} - \\mathbf{\\mu_{2}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{2}}) = \\mathbf{x}^{T}\\Sigma^{-1}\\mathbf{x} - 2\\mathbf{x}^{T}\\Sigma^{-1}\\mathbf{\\mu_{2}} + \\mathbf{\\mu_{2}}^{T}\\Sigma^{-1}\\mathbf{\\mu_{2}}$.\n",
    "\n",
    "Thus we can expand out:\n",
    "\n",
    "> $(\\mathbf{x} - \\mathbf{\\mu_{1}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{1}}) -(\\mathbf{x} - \\mathbf{\\mu_{2}})^{T} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu_{2}})$\n",
    "\n",
    "to\n",
    "\n",
    "> $- 2\\mathbf{x}^{T}\\Sigma^{-1}\\mathbf{\\mu_{1}} + \\mathbf{\\mu_{1}}^{T}\\Sigma^{-1}\\mathbf{\\mu_{1}} + 2\\mathbf{x}^{T}\\Sigma^{-1}\\mathbf{\\mu_{2}} - \\mathbf{\\mu_{2}}^{T}\\Sigma^{-1}\\mathbf{\\mu_{2}}$,\n",
    "\n",
    "where we can factor out the $-2\\mathbf{x}^{T}\\Sigma^{-1}$ terms:\n",
    "\n",
    "> $-2\\mathbf{x}^{T}\\Sigma^{-1}(\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{2}}) + \\mathbf{\\mu_{1}}^{T}\\Sigma^{-1}\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{2}}^{T}\\Sigma^{-1}\\mathbf{\\mu_{2}}$.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The $\\mathbf{\\mu_{1}}^{T}\\Sigma^{-1}\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{2}}^{T}\\Sigma^{-1}\\mathbf{\\mu_{2}}$ terms follow the difference of two squares, and can be written as $(\\mathbf{\\mu_{1}} + \\mathbf{\\mu_{2}})^{T}\\Sigma^{-1}(\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{2}})$.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Our log-likelihood can therefore be written:\n",
    "\n",
    "> $\\ln \\left ( \\dfrac{f_{1}(\\mathbf{x})}{f_{2}(\\mathbf{x})} \\right ) = -\\dfrac{1}{2} \\left ( -2\\mathbf{x}^{T}\\Sigma^{-1}(\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{2}}) + (\\mathbf{\\mu_{1}} + \\mathbf{\\mu_{2}})^{T}\\Sigma^{-1}(\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{2}}) \\right )$\n",
    "\n",
    "> $= \\mathbf{x}^{T}\\Sigma^{-1}(\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{2}}) -\\dfrac{1}{2}(\\mathbf{\\mu_{1}} + \\mathbf{\\mu_{2}})^{T}\\Sigma^{-1}(\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{2}})$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The common factor in both terms is $\\Sigma^{-1}(\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{2}})$. Let $\\mathbf{L} = \\Sigma^{-1}(\\mathbf{\\mu_{1}} - \\mathbf{\\mu_{2}})$ such that our log-likelihood takes the form:\n",
    "\n",
    "> $\\ln \\left ( \\dfrac{f_{1}(\\mathbf{x})}{f_{2}(\\mathbf{x})} \\right ) =  \\mathbf{x}^{T}\\mathbf{L} -\\dfrac{1}{2}(\\mathbf{\\mu_{1}} + \\mathbf{\\mu_{2}})^{T}\\mathbf{L}$.\n",
    "\n",
    "Note again that each of the two above terms is a scalar ($\\mathbf{L}$ is a $p \\times 1$ vector), so we can shuffle the order of the terms by taking the transpose of each:\n",
    "\n",
    "> $\\ln \\left ( \\dfrac{f_{1}(\\mathbf{x})}{f_{2}(\\mathbf{x})} \\right ) =  \\mathbf{L}^{T}\\mathbf{x} -\\dfrac{1}{2}\\mathbf{L}^{T}(\\mathbf{\\mu_{1}} + \\mathbf{\\mu_{2}})$.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Recall that as:\n",
    "\n",
    "> $\\dfrac{f_{1}(\\mathbf{x})}{f_{2}(\\mathbf{x})} \\geq \\dfrac{\\pi_{2} C(1|2)}{\\pi_{1}C(2|1)}$,\n",
    "\n",
    "the log-likelihood relationship must be:\n",
    "\n",
    "> $\\ln \\left ( \\dfrac{f_{1}(\\mathbf{x})}{f_{2}(\\mathbf{x})} \\right ) \\geq \\ln \\left ( \\dfrac{\\pi_{2} C(1|2)}{\\pi_{1}C(2|1)} \\right )$,\n",
    "\n",
    "therefore:\n",
    "\n",
    "> $\\mathbf{L}^{T}\\mathbf{x} -\\dfrac{1}{2}\\mathbf{L}^{T}(\\mathbf{\\mu_{1}} + \\mathbf{\\mu_{2}}) \\geq \\ln \\left ( \\dfrac{\\pi_{2} C(1|2)}{\\pi_{1}C(2|1)} \\right )$.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "**Our allocation rule is therefore:**\n",
    "\n",
    "**Allocate to population 1 if:**\n",
    "\n",
    "> $\\mathbf{L}^{T}\\mathbf{x} -\\dfrac{1}{2}\\mathbf{L}^{T}(\\mathbf{\\mu_{1}} + \\mathbf{\\mu_{2}}) \\geq \\ln \\left ( \\dfrac{\\pi_{2} C(1|2)}{\\pi_{1}C(2|1)} \\right )$,\n",
    "\n",
    "**otherwise allocate to population 2.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Mahalanobis distance and misclassification probability.**\n",
    "\n",
    "We are provided with the mean results for four tests administered to two groups.\n",
    "\n",
    "We are also provided with the pooled *within-groups* sample covariance matrix, $\\mathbf{S}_{U}$, which is an unbiased estimator for the population covariance matrix, $\\Sigma$, whereby:\n",
    "\n",
    "> $\\mathbf{S}_{U} = \\dfrac{(n_{1}-1)\\mathbf{S}_{1U} + (n_{2}-1)\\mathbf{S}_{2U}}{n_{1} + n_{2} - 2}$.\n",
    "\n",
    "The squared Mahalanobis distance (a measure of distance between two **population** means), $\\alpha$, is calculated as:\n",
    "\n",
    "> $\\alpha = (\\mu_{1} - \\mu_{2})^{T}\\Sigma^{-1}(\\mu_{1} - \\mu_{2})$.\n",
    "\n",
    "Since we don't know the population paramaters, we can calculate the sample squared Mahalanobis distance:\n",
    "\n",
    "> $\\hat{\\alpha} = (\\bar{\\mathbf{x}}_{1} - \\bar{\\mathbf{x}}_{2})^{T}\\mathbf{S}_{U}^{-1}(\\bar{\\mathbf{x}}_{1} - \\bar{\\mathbf{x}}_{2})$.\n",
    "\n",
    "We therefore take the inverse of the pooled sample covariance matrix, $\\mathbf{S}_{U}^{-1}$, which is provided as **invSp** in the question, and calculate $\\hat{\\alpha}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 4 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>subtest</th><th scope=col>senile.37</th><th scope=col>non.senile.12</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Information       </td><td>12.57</td><td>8.75</td></tr>\n",
       "\t<tr><td>Similarities      </td><td> 9.57</td><td>5.35</td></tr>\n",
       "\t<tr><td>Arithmetic        </td><td>11.49</td><td>8.50</td></tr>\n",
       "\t<tr><td>Picture completion</td><td> 7.97</td><td>4.75</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 4 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       " subtest & senile.37 & non.senile.12\\\\\n",
       " <fct> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t Information        & 12.57 & 8.75\\\\\n",
       "\t Similarities       &  9.57 & 5.35\\\\\n",
       "\t Arithmetic         & 11.49 & 8.50\\\\\n",
       "\t Picture completion &  7.97 & 4.75\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 4 × 3\n",
       "\n",
       "| subtest &lt;fct&gt; | senile.37 &lt;dbl&gt; | non.senile.12 &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| Information        | 12.57 | 8.75 |\n",
       "| Similarities       |  9.57 | 5.35 |\n",
       "| Arithmetic         | 11.49 | 8.50 |\n",
       "| Picture completion |  7.97 | 4.75 |\n",
       "\n"
      ],
      "text/plain": [
       "  subtest            senile.37 non.senile.12\n",
       "1 Information        12.57     8.75         \n",
       "2 Similarities        9.57     5.35         \n",
       "3 Arithmetic         11.49     8.50         \n",
       "4 Picture completion  7.97     4.75         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample mean vector for senile group\n",
    "senile.37 <- c(12.57, 9.57, 11.49, 7.97)\n",
    "\n",
    "# sample mean vector for non-senile group\n",
    "non.senile.12 <- c(8.75, 5.35, 8.50, 4.75)\n",
    "\n",
    "# the four different tests performed by each individual\n",
    "subtest <- c('Information','Similarities','Arithmetic','Picture completion')\n",
    "\n",
    "df.means <- data.frame(subtest, senile.37, non.senile.12)\n",
    "\n",
    "df.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 4 × 4 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Information</th><td> 0.25907356</td><td>-0.13576450</td><td>-0.05877998</td><td>-0.06473009</td></tr>\n",
       "\t<tr><th scope=row>Similarities</th><td>-0.13576450</td><td> 0.18645117</td><td>-0.03833003</td><td> 0.01438476</td></tr>\n",
       "\t<tr><th scope=row>Arithmetic</th><td>-0.05877998</td><td>-0.03833003</td><td> 0.15098314</td><td>-0.01694172</td></tr>\n",
       "\t<tr><th scope=row>PictureCompletion</th><td>-0.06473009</td><td> 0.01438476</td><td>-0.01694172</td><td> 0.21117177</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 4 × 4 of type dbl\n",
       "\\begin{tabular}{r|llll}\n",
       "\tInformation &  0.25907356 & -0.13576450 & -0.05877998 & -0.06473009\\\\\n",
       "\tSimilarities & -0.13576450 &  0.18645117 & -0.03833003 &  0.01438476\\\\\n",
       "\tArithmetic & -0.05877998 & -0.03833003 &  0.15098314 & -0.01694172\\\\\n",
       "\tPictureCompletion & -0.06473009 &  0.01438476 & -0.01694172 &  0.21117177\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 4 × 4 of type dbl\n",
       "\n",
       "| Information |  0.25907356 | -0.13576450 | -0.05877998 | -0.06473009 |\n",
       "| Similarities | -0.13576450 |  0.18645117 | -0.03833003 |  0.01438476 |\n",
       "| Arithmetic | -0.05877998 | -0.03833003 |  0.15098314 | -0.01694172 |\n",
       "| PictureCompletion | -0.06473009 |  0.01438476 | -0.01694172 |  0.21117177 |\n",
       "\n"
      ],
      "text/plain": [
       "                  [,1]        [,2]        [,3]        [,4]       \n",
       "Information        0.25907356 -0.13576450 -0.05877998 -0.06473009\n",
       "Similarities      -0.13576450  0.18645117 -0.03833003  0.01438476\n",
       "Arithmetic        -0.05877998 -0.03833003  0.15098314 -0.01694172\n",
       "PictureCompletion -0.06473009  0.01438476 -0.01694172  0.21117177"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# symmetric sample covariance matrix\n",
    "sample.covar <- data.frame(Information = c(11.2553,9.4042,7.1489,3.3830),\n",
    "                 Similarities = c(9.4042,13.5318,7.3830,2.5532),\n",
    "                 Arithmetic = c(7.1489,7.3830,11.5744,2.6170),\n",
    "                 PictureCompletion = c(3.3830,2.5532,2.6170,5.8085))\n",
    "\n",
    "sample.covar.inv <- solve(sample.covar)\n",
    "\n",
    "sample.covar.inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared Mahalanobis distance calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2.42533274943634"
      ],
      "text/latex": [
       "2.42533274943634"
      ],
      "text/markdown": [
       "2.42533274943634"
      ],
      "text/plain": [
       "[1] 2.425333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha.hat <- t(senile.37 - non.senile.12) %*% sample.covar.inv %*% (senile.37 - non.senile.12)\n",
    "\n",
    "alpha.hat[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the squared Mahalanobis distance to be $2.43$.\n",
    "\n",
    "The probability that an individual is misallocated to the correct group, i.e. is misclassified, is given by the standard normal distribution function $= \\Phi \\left ( - \\dfrac{\\sqrt{\\hat{\\alpha}}}{2} \\right )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.218085889888665"
      ],
      "text/latex": [
       "0.218085889888665"
      ],
      "text/markdown": [
       "0.218085889888665"
      ],
      "text/plain": [
       "[1] 0.2180859"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pnorm(-0.5 * sqrt(alpha.hat[1,1]), lower.tail = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We calculate a misclassification probability of $0.22$.**\n",
    "\n",
    "Note, this probability may be *underestimated* for small sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) PCA\n",
    "\n",
    "*100 individuals were given 3 scores based on a series of tests designed to measure an underlying attitude.*\n",
    "\n",
    "(i) The third eigenvector has an eigenvalue of zero. This means there's an exact linear relationship between the three variables (the third eigenvector not being orthogonal to the other two). The third principal component accounts for *zero variance* with respect to the other two, and therefore that one of the variables is redundant.\n",
    "\n",
    "(ii) The first principal component accounts for $\\dfrac{296.724}{296.724 + 25.276} = 92\\%$ of the variance. This dominating component is a measure of the overall size , with all three coefficients having similar magnitude and *sign*.\n",
    "\n",
    "The second principal component accounts for the remaining $8\\%$ of the variance (as the third component accounts for zero variance), but interestingly, shows there to be a *contrast between the second and third variables*, with opposite sign, whilst the first variable has no impact.\n",
    "\n",
    "There's therefore interest in the interplay between the second and third variables, and since one of the variables should be removed, it makes sense that it's the first. \n",
    "\n",
    "When re-running the PCA on the second and third variable, you'd be running PCA on the following unbiased sample covariance matrix:\n",
    "\n",
    "> $\\mathbf{S}_{U} = \\begin{bmatrix}\n",
    "52 & 36\\\\ \n",
    "36 & 73\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Where the expression to find the eigenvalues, indicating the proportions of the variance accounted for by each of the two principal components is:\n",
    "\n",
    "> $\\det\\begin{bmatrix}\n",
    "52 - \\lambda & 36\\\\ \n",
    "36 & 73 - \\lambda\n",
    "\\end{bmatrix} = 0$\n",
    "\n",
    "I'd expect there to still be an overall size component as the first principal component that accounts for most of the variance, where each coefficient has the same sign. I'd then expect the second component, with a significantly lower eigenvalue / variance, to contain coefficients with the **same coefficient values, but of opposite sign**.\n",
    "\n",
    "This is because we're down to two variables and two principal components, where each component is a linear combination of the two variables, the size of the weights for each variable in the two linear combinations won't change. But, the sign will in the second principal component, as we've seen there to be a contrasting interaction between the second and third variables when performing PCA initially, with the three variables.\n",
    "\n",
    "This initial variable removal process of finding and removing highly correlated variables is useful as a pre-processing step prior to fitting a regression. Highly correlated variables can cause enormous errors in regression coefficients, which is clearly sub-optimal.\n",
    "\n",
    "Note that by reducing the number of variables, we've reduced the dimensionality of our data, from three dimensions to two, by *eliminating* one of our underlying variables. This is one of the two ways in which PCA can be used to reduce the dimenions in our data. The other way, is to select the principal components that explain most of the variance in the data, where these components represent linear combinations of the underlying variables, where we can treat these principal components as 'new' variables, and hence work with fewer number of principal components than our original number of variables, whilst maintaining most of the variance in the data in this new representation.\n",
    "\n",
    "As we've also reduced the number of our variable dimensions from 3 to 2, we can more easily visually interpret the interplay between the two variables in a plot (I find 3-D plots often tricky to interpret!), as the PCA analysis has revealed the best 2-D projection of the test score data.\n",
    "\n",
    "This iterative PCA process could also help to feedback to the test designers that the three scores that they've produced aren't truly orthogonal when it comes to measuring the underlying attitude - essentially one of those tests has been wasted.\n",
    "\n",
    "As an interesting aside, I suspect the PCA analysis from the above would look similar to the PCA analysis that would result from comparing the returns of two similar stocks in a paired analysis: e.g. Coke and Pepsi. The results of which are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 5 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>return_coke</th><th scope=col>return_pepsi</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>-0.003780718</td><td>-0.002890173</td></tr>\n",
       "\t<tr><td>-0.001897533</td><td>-0.014492754</td></tr>\n",
       "\t<tr><td>-0.008593156</td><td>-0.008823529</td></tr>\n",
       "\t<tr><td>-0.004755695</td><td>-0.005934718</td></tr>\n",
       "\t<tr><td> 0.001233141</td><td> 0.000000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       " return\\_coke & return\\_pepsi\\\\\n",
       " <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t -0.003780718 & -0.002890173\\\\\n",
       "\t -0.001897533 & -0.014492754\\\\\n",
       "\t -0.008593156 & -0.008823529\\\\\n",
       "\t -0.004755695 & -0.005934718\\\\\n",
       "\t  0.001233141 &  0.000000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 × 2\n",
       "\n",
       "| return_coke &lt;dbl&gt; | return_pepsi &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| -0.003780718 | -0.002890173 |\n",
       "| -0.001897533 | -0.014492754 |\n",
       "| -0.008593156 | -0.008823529 |\n",
       "| -0.004755695 | -0.005934718 |\n",
       "|  0.001233141 |  0.000000000 |\n",
       "\n"
      ],
      "text/plain": [
       "  return_coke  return_pepsi\n",
       "1 -0.003780718 -0.002890173\n",
       "2 -0.001897533 -0.014492754\n",
       "3 -0.008593156 -0.008823529\n",
       "4 -0.004755695 -0.005934718\n",
       "5  0.001233141  0.000000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataframe with returns for Coca Cola and Pepsi going back to the 1960s.\n",
    "df.pair <- read.csv('coke_pepsi_paired_returns.csv')\n",
    "\n",
    "df.pair[1:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 2 × 2 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>return_coke</th><th scope=col>return_pepsi</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>return_coke</th><td>0.0002328181</td><td>0.0001219119</td></tr>\n",
       "\t<tr><th scope=row>return_pepsi</th><td>0.0001219119</td><td>0.0002464368</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 2 × 2 of type dbl\n",
       "\\begin{tabular}{r|ll}\n",
       "  & return\\_coke & return\\_pepsi\\\\\n",
       "\\hline\n",
       "\treturn\\_coke & 0.0002328181 & 0.0001219119\\\\\n",
       "\treturn\\_pepsi & 0.0001219119 & 0.0002464368\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 2 × 2 of type dbl\n",
       "\n",
       "| <!--/--> | return_coke | return_pepsi |\n",
       "|---|---|---|\n",
       "| return_coke | 0.0002328181 | 0.0001219119 |\n",
       "| return_pepsi | 0.0001219119 | 0.0002464368 |\n",
       "\n"
      ],
      "text/plain": [
       "             return_coke  return_pepsi\n",
       "return_coke  0.0002328181 0.0001219119\n",
       "return_pepsi 0.0001219119 0.0002464368"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# producing the sample covariance matrix\n",
    "S <- var(df.pair)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eigen() decomposition\n",
       "$values\n",
       "[1] 0.0003617293 0.0001175255\n",
       "\n",
       "$vectors\n",
       "          [,1]       [,2]\n",
       "[1,] 0.6871070 -0.7265563\n",
       "[2,] 0.7265563  0.6871070\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# producing the eigenvalues and eigenvectors\n",
    "eig <- eigen(S)\n",
    "eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>75.4774469270531</li>\n",
       "\t<li>24.5225530729469</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 75.4774469270531\n",
       "\\item 24.5225530729469\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 75.4774469270531\n",
       "2. 24.5225530729469\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 75.47745 24.52255"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# proportion of variance between the first and second principal components\n",
    "(eig$values/sum(eig$values)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**END OF ASSIGNMENT**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
